let video
let canvas

const meter = new Tone.Meter({ smoothing: 0 })
const synth = new Tone.Synth().toDestination()
synth.connect(meter)

function attack () {
  const freq = 440
  synth.triggerAttack(freq)
}

function release () {
  synth.triggerRelease()
}

nn.create('button')
  .content('hold')
  .addTo('body')
  .on('pointerdown', attack)
  .on('pointerup', release)
  .on('pointerleave', release)

// create video element and canvas element
async function setup () {
  // create a hidden video element with our camera feed
  video = nn.create('video')
    // .addTo('body') // <- omit this, to keep it hidden
    .set({
      autoplay: true,
      muted: true,
      stream: await nn.askFor({ video: true })
    })

  // create canvas, where we'll draw video feed
  canvas = nn.create('canvas')
    .css('display', 'block')
    .addTo('body')
}

// check if the video is ready
// if so make sure canvas size matches video size
function ready () {
  if (!video || video.videoWidth === 0) return false
  else if (video.videoWidth !== canvas.width) {
    canvas.width = video.videoWidth
    canvas.height = video.videoHeight
  }
  return true
}

// animation loop, update canvas with modified video
function draw () {
  requestAnimationFrame(draw)
  if (!ready()) return // skip frame if not(!)ready

  // draw video frame to canvas
  canvas.drawImage(video, 0, 0)

  // get volume level (convert db to linear level)
  const db = meter.getValue()
  const lvl = Tone.dbToGain(db)

  // loop over every pixel in this frame
  let pixels = canvas.getPixels()
  for (let i = 0; i < pixels.length; i++) {
    const p = pixels[i] // get this pixel
    // brighten red channel if volume is loud
    //      blend  A, B by factor (0-1)
    p.r = nn.lerp(p.r, 255, lvl)
  }
  canvas.setPixels(pixels)
}

nn.on('load', setup)
nn.on('load', draw)
