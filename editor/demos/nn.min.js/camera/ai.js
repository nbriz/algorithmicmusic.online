/*
  In this example we're using the [Tensorflow.js](https://www.tensorflow.org/js/) machine learning (AI) library to load the [BlazePose](https://github.com/tensorflow/tfjs-models/tree/master/pose-detection#blazepose) AI model which can detect the position of various ["keypoints"](https://github.com/tensorflow/tfjs-models/tree/master/pose-detection#blazepose-keypoints-used-in-mediapipe-blazepose) in a person's pose. While Tensorflow can be used to create or train your own AI neural networks, there are also [many pre-trained models](https://www.tensorflow.org/js/models) like BlazePose to choose from.
*/

// global variables
let detector, video, osc, volEle, freqEle

// this function loads the AI model
async function setupModel () {
  // here we pick which pre-trained model we want to use
  const model = poseDetection.SupportedModels.BlazePose
  // here we setup some "configuration" settings
  const detectorConfig = {
    runtime: 'mediapipe',
    solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/pose'
  }
  // we combine the two to create the AI "detector" function
  const detector = await poseDetection.createDetector(model, detectorConfig)
  return detector
}

// our update function (a recursive looping function)
async function update () {
  // we first use the detector AI function to predict our "pose" based on the video frame
  const poses = await detector.estimatePoses(video)
  // we'll store the keypoints for the first person it sees every frame
  const keypoints = poses[0]?.keypoints

  if (keypoints) {
    // we grab the 19 (left_index) and 20 (right_index) keypoints, we can see which keypoint indexes refer to which part of the pose [using this BlazePose diagram](https://github.com/tensorflow/tfjs-models/tree/master/pose-detection#blazepose-keypoints-used-in-mediapipe-blazepose)
    const left = keypoints[19]
    const right = keypoints[20]

    // Adjust volume based on left index finger position
    osc.volume.value = (left.y >= 0 && left.y <= 480)
      ? nn.map(left.y, 0, 480, 0, -50) : -100
    // update the visual reference as well
    volEle.css({ top: `${left.y}px`, left: `${left.x}px` })
    // Adjust frequency based on right index finger position
    osc.frequency.value = nn.map(right.y, 0, 480, 220, 880)
    // update the visual reference as well
    freqEle.css({ top: `${right.y}px`, left: `${right.x}px` })
  } else {
    osc.volume.value = -100; // Mute if no pose is detected
  }

  window.requestAnimationFrame(update)
}

async function setup () {
  // we create (&& begin playing) the Tone.js Oscillator
  osc = new Tone.Oscillator().toDestination().start()
  osc.volume.value = -100 // set it's volume down by default

  // here we create the visual elements
  volEle = nn.create('div')
    .css({ color: 'red', fontSize: '48px', position: 'absolute' })
    .content('VOL')
    .addTo('body')

  freqEle = nn.create('div')
    .css({ color: 'red', fontSize: '48px', position: 'absolute' })
    .content('FREQ')
    .addTo('body')

  // create the video element with our camera feed
  video = nn.create('video')
    .addTo('body')
    .set({
      autoplay: true,
      muted: true,
      stream: await nn.askFor({ video: true })
    })

  // then we create our AI function
  detector = await setupModel()

  // run the update function to start the recursive loop
  update()
}

nn.on('load', setup)
