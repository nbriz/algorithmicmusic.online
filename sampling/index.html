<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Algorithmic Music Online: Sampling</title>
    <meta name="author" content="Nick Briz">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- TODO: social media stuff -->
    <link rel="stylesheet" href="/css/main.css">
    <style>
      #search-audio-files {
        max-width: 1300px;
        max-height: 80vh;
        margin: 0 auto;
        border: 1px solid black;
        padding: 10px;
        display: grid;
        grid-template-columns: 1fr 1fr 3fr;
      }

      #search-audio-files > div {
        max-height: calc(80vh - 20px);
        overflow-y: scroll;
      }

      .clickable, #af3 {
        font-family: 'FiraMono', inconsolata, monospace;
        padding: 4px 10px;
      }

      .clickable:hover {
        cursor: pointer;
        color: var(--accent-color1);
      }

      #waveform-sampler {
        display: flex;
        flex-direction: column;
        align-items: center;
        margin: 30px auto;
      }
    </style>
  </head>
  <body>

    <main-menu>
      <div slot="menu-items">
        <div class="color-mode-wrap">
          <label class="color-mode-switch">
            <input type="checkbox">
            <span class="color-mode-slider"></span>
          </label>
          <span class="color-mode-label">light/dark mode</span>
        </div>
      </div>
    </main-menu>

    <div class="loader">
      <svg id="loader-wave" viewBox="0 0 100 100" preserveAspectRatio="none">
        <path d="" id="loader-wave-path"></path>
      </svg>
    </div>

    <section>
      <div class="content">
        <h4 class="ch">chapter 3</h4>
        <h2 class="formatted-text ch-title">sampling</h2>

        <p>
          In music production, "sampling" can mean a few different things depending on the context. This word has already shown up in the previous lessons when creating audio buffers. When creating sound from raw audio buffers we must specify how many channels (individual speakers) we have as well as how many seconds long we want the buffer to be, to calculate this we must determine our sample rate typically 44,100 samples per second or 44.1 kHz. In this context a "sample" refers to a single value (between -1 and 1) representing the position of the speaker (generating the vibration) at that point in time.
        </p>

        <p>
          In our previous examples we've generated audio buffers algorithmically, in this lesson we'll be creating buffers by loading data from previously recorded sounds. We can do this with the Web Audio API (see <a href="/editor/#Web Audio API/Audio Buffers/From File" target="_blank">Web Audio API / Audio Buffers / From File</a>), but in the exmples below we'll be using Tone.js to keep things "higher level". When we record analog sounds and digitize them, we are again creating audio buffers except this time by "sampling" from the analog world. In this context, "sampling" is a purely technical process that transforms continuous analog sound into the digital domain by slicing it into tiny snapshots (again, values between -1 and 1), each representing the amplitude of the sound wave at a specific moment in time.
        </p>

        <div style="max-width: 80%; margin: 0 auto;">
          <a href="https://www.youtube.com/watch?v=5SaFTm2bcac" target="_blank">
            <img src="/images/amen-yt.jpg" alt="amen break record" class="rwd">
          </a>
          <p class="small-note">
            an audio documentary on the "<a href="/audios/amen-break.mp3" target="_blank">Amen Break</a>"
          </p>
        </div>

        <p>
          More broadly speaking, <strong>sampling</strong> generally refers to the process of taking a portion, or "sample," of an existing sound or audio recording and reusing it in a new context. This could involve lifting a short segment of a song—like a drum break, melody, or vocal phrase—and incorporating it into a new composition, often altering it with effects, pitch shifts, or looping.
        </p>

      </div>
    </section>

    <section id="editor-1"></section>

    <section id="waveform-sampler"></section>

    <section>
      <div class="content">
        <p>
          Above is an interactive tool you can use to load any audio clip and set start/stop loop points. This can be used to more easily identify the offset value (how many seconds into the audio file do we want to start our sample) and duration (how many seconds after that offset should we stop our sample) for us to pass into our player's <code>start()</code> method.
        </p>
      </div>
    </section>

    <br><br><hr><br><br><br>

    <section>
      <div class="content">
        <div style="max-width: 80%; margin: 0 auto;">
          <a href="https://www.youtube.com/watch?v=qsRuhCflRyg" target="_blank">
            <img src="/images/delia-yt.jpg" alt="delia derbyshire" class="rwd">
          </a>
          <p class="small-note">
            <a href="https://en.wikipedia.org/wiki/Delia_Derbyshire" target="_blank">Delia Derbyshire</a> early pioneer in sampling and electronic music. you can watch a longer <a href="https://www.youtube.com/watch?v=nXnmSgaeGAI" target="_blank">documentary about her and her work here</a>.
          </p>
        </div>

        <p>
          Another meaning of sampling appears in synthesis, where a digital sampler is used to capture and manipulate sound waves, allowing them to be played back at different pitches and dynamics, similar to how an instrument is played. In the example below I'm using the <a href="https://tonejs.github.io/docs/15.0.4/classes/Sampler.html" target="_blank">Tone.Sampler</a>, like the Synth we discussed in the last lesson, the Sampler is a Tone.js "Instrument" for creating and playing back sampled sounds. So rather than using the <code>start()</code> and <code>stop()</code> methods it has the <code>triggerAttack</code>, <code>triggerRelease</code> and <code>triggerAttackRelease</code> methods. The Sampler maps audio files to specific pitches, allowing you to create instruments where each key triggers a different sample or stretches a single sample across multiple keys.
        </p>
      </div>
    </section>

    <section id="editor-2"></section>

    <br><br><br>

    <section>
      <div class="content">
        <p>
          <p>
            You can record sounds and create your own audio files for sampling purposes, but it's often easier to start with some sound files others have made. The <code>baseURL</code> in the last example above points to a Tone.js domain, this isn't part of the Tone.js library per se, it's actually a <a href="https://github.com/Tonejs/audio/tree/master" target="_blank">repository of audio files</a> the creators of Tone.js started with samples they use to write examples. While I highly recommend you search large open source sound repositories, like <a href="https://freesound.org/" target="_blank">freesound.org</a> or <a href="https://commons.wikimedia.org/wiki/Category:Audio_files" target="_blank">Wiki Media</a>, for interesting sounds (or even YouTube via sites like <a href="https://www.y2mp3.biz/" target="_blank">this one</a>), the samples provided by the creators of Tone.js can be an easy/quick place to start experimenting. Below I've created a tool for quickly searching the repository.
          </p>
        </p>
      </div>
    </section>

    <section id="search-audio-files">
      <div id="af1"></div>
      <div id="af2"></div>
      <div id="af3"></div>
    </section>

    <section class="attribution">
      <div class="content">
        <p>
          <span style="font-weight: bold;">Attribution</span>: Text and code written by <a href="https://nickbriz.com/" target="_blank">Nick Briz</a>. The code editor icons designed by <a href="https://thenounproject.com/creator/MekoDa/" target="_blank">Meko</a> and licensed under Creative Commons Attribution License (CC BY 3.0). All sounds generated using the Web Audio API and/or <a href="https://tonejs.github.io/" target="_blank">Tone.js</a> by Yotam Mann and <a href="https://github.com/Tonejs/Tone.js/graphs/contributors" target="_blank">other contributors</a>.
        </p>
      </div>
    </section>

    <!-- <script src="/js/libs/d3@7.js"></script> -->
    <!-- <script src="/js/libs/Tone.js"></script> -->
    <script src="/js/libs/netitor.min.js"></script>
    <script src="/js/libs/nn.min.js"></script>
    <script src="/js/custom-elements/main-menu.js"></script>
    <!-- <script src="/js/custom-elements/adsr-ui.js"></script> -->
    <!-- <script src="/js/custom-elements/algo-music-ui.js"></script> -->
    <!-- <script src="/js/jack/sound_wave.js"></script> -->
    <!-- <script src="/js/jack/sound-1.js"></script> -->
    <!-- <script src="/js/SVGSineWave.js"></script> -->
    <!-- <script src="/js/create-waveform.js"></script> -->
    <!-- <script src="/js/synth-presets.js"></script> -->
    <!-- <script src="/js/effect-presets.js"></script> -->
    <!-- <script src="/js/create-spectrum.js"></script> -->
    <script src="/js/tone-sounds.js"></script>
    <script src="/js/code-templates.js"></script>
    <script src="/js/utils.js"></script>
    <script>
      utils.init()
      nn.getAll('h4, .formatted-text').forEach(e => utils.formatText(e))

      utils.createCodeEditor({
        ele: '#editor-1',
        title: 'sampling basics',
        fileprefix: 'sampling-',
        template: [3, 3, 3],
        total: 3,
        index: 1
      })

      utils.createCodeEditor({
        ele: '#editor-2',
        title: 'the "sampler"',
        fileprefix: 'sampler-',
        template: [3, 3, 3],
        total: 3,
        index: 1
      })

      // -----------------------------------------------------------------------
      // -----------------------------------------------------------------------
      // -----------------------------------------------------------------------

      function createAudioSearch () {
        let selected = ''
        const info = {
          berklee: 'Files from the <a href="http://wiki.laptop.org/go/Sound_samples" target="_blank">OLPC Berklee Sound Library</a>, Licensed under CC BY 3.0',
          casio: 'Samples by Yotam Mann 2015, <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank"> Licensed under CC BY 4.0</a>',
          'drum-samples': 'Samples taken from Chris Wilson <a href="https://github.com/cwilso/web-audio-samples" target="_blank">web audio samples</a>',
          'impulse-responses': 'Samples taken from Chris Wilson <a href="https://github.com/cwilso/web-audio-samples" target="_blank">web audio samples</a> (except for berlin_tunnel_ir.wav by Liberti & Rife LLC)',
          loop: 'Samples by Yotam Mann 2015, <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank"> Licensed under CC BY 4.0</a>',
          salamander: 'Salamander Grand Piano V2, Yamaha C5 by Alexander Holm <a href="http://creativecommons.org/licenses/by/3.0/" target="_blank"> Licensed under CC BY 3.0</a>'
        }

        const clickDir = (files, dir) => {
          nn.get('#af2').innerHTML = ''
          nn.get('#af3').innerHTML = ''
          if (dir === 'drum-samples') setupDrums()
          files.forEach(file => {
            nn.create('div')
              .set('class', 'clickable')
              .css({ color: 'var(--accent-color4)' })
              .content(file)
              .addTo('#af2')
              .on('click', () => updateObj(file))
          })
        }

        const updateObj = (file) => {
          const i = selected.split('/')[0]
          const url = `https://tonejs.github.io/audio/${selected}/${file}`
          nn.get('#af3').innerHTML = ''
          nn.create('p').addTo('#af3').content(info[i])
          nn.create('audio').set({ src: url, controls: true }).addTo('#af3')
          nn.create('pre').addTo('#af3').innerHTML = `{
  url: ${url}
  baseUrl: https://tonejs.github.io/audio/${selected},
  filename: ${file}
}`
        }

        const setupDrums = () => {
          const ds = window.toneSounds['drum-samples']
          for (const kit in ds) {
            if (kit !== 'files') {
              const o = ds[kit]
              console.log(kit);
              nn.create('div')
                .set('class', 'clickable')
                .content(kit)
                .addTo('#af2')
                .on('click', () => {
                  selected += `/${kit}`
                  clickDir(o.files)
                })
            }
          }
        }

        for (const prop in window.toneSounds) {
          const dir = window.toneSounds[prop]
          nn.create('div')
            .set('class', 'clickable')
            .content(prop)
            .addTo('#af1')
            .on('click', () => {
              selected = prop
              clickDir(dir.files, prop)
            })
        }
      }

      nn.on('load', createAudioSearch)

      // -----------------------------------------------------------------------
      // -----------------------------------------------------------------------
      // -----------------------------------------------------------------------

      function createWFSampler (opts) {
        opts = opts || { ele: 'body', color: 'black' }

        const handleAudioFile = (file) => {
          const reader = new window.FileReader()
          reader.onload = (e) => {
            audioCtx.decodeAudioData(e.target.result).then((buffer) => {
              audioBuffer = buffer
              loopInTime = 0
              loopOutTime = audioBuffer.duration
              loopInX = 0
              loopOutX = canvas.width
              btn.css({ display: 'inline' })
              loopStart.css({ display: 'block', left: '0px', top: `${canvas.offsetTop}px` })
              loopEnd.css({ display: 'block', left: '0px', top: `${canvas.offsetTop}px` })
              drawWaveform()
            }).catch((error) => console.error('Error decoding audio data:', error))
          }
          reader.readAsArrayBuffer(file)
        }

        const drawWaveform = () => {
          const width = canvas.width
          const height = canvas.height
          const data = audioBuffer.getChannelData(0)
          const step = Math.ceil(data.length / width)
          const amp = height / 2

          ctx.clearRect(0, 0, width, height)
          ctx.fillStyle = 'transparent'
          ctx.fillRect(0, 0, width, height)

          ctx.strokeStyle = opts.colors.wave
          ctx.beginPath()

          for (let i = 0; i < width; i++) {
            let min = 1.0
            let max = -1.0

            for (let j = 0; j < step; j++) {
              const datum = data[(i * step) + j]
              if (datum < min) min = datum
              if (datum > max) max = datum
            }

            ctx.moveTo(i, (1 + min) * amp)
            ctx.lineTo(i, (1 + max) * amp)
          }

          ctx.stroke()

          // Draw Loop Points
          // drawLoopPoints()
        }

        const playAudio = (file) => {
          player = audioCtx.createBufferSource()
          player.buffer = audioBuffer
          player.loop = true
          player.loopStart = loopInTime
          player.loopEnd = loopOutTime
          player.connect(audioCtx.destination)

          const offset = pausedTime || loopInTime
          player.start(0, offset)
          startTime = audioCtx.currentTime - offset
          isPlaying = true
          btn.textContent = 'pause'

          const y  = `${canvas.offsetTop}px`
          loopStart.css({ top: y })
          loopArea.css({ top: y })
          loopEnd.css({ top: y })
          animatePlayhead()
        }

        const pauseAudio = () => {
          if (player) player.stop()
          pausedTime = audioCtx.currentTime - startTime
          isPlaying = false
          btn.textContent = 'play'
          cancelAnimationFrame(animationFrameId)
        }

        const animatePlayhead = () => {
          if (!isPlaying) return

          const currentTime = audioCtx.currentTime - startTime
          const duration = audioBuffer.duration
          let position = currentTime % (loopOutTime - loopInTime) + loopInTime
          if (position > loopOutTime) {
            position = loopInTime
            startTime = audioCtx.currentTime - loopInTime
          }

          const x = (position / duration) * canvas.width
          playhead.css({ left: `${x}px`, top: `${canvas.offsetTop}px` })
          animationFrameId = requestAnimationFrame(animatePlayhead)
        }

        // .....
        const audioCtx = new (window.AudioContext || window.webkitAudioContext)()
        let audioBuffer, player, animationFrameId
        let isPlaying = false
        let startTime = 0
        let pausedTime = 0
        let loopInTime = 0
        let loopOutTime = 0

        const transport = nn.create('div')
          .css({ display: 'flex', 'justify-content': 'center', 'align-items': 'center' })
          .addTo(opts.ele)

        const btn = nn.create('button')
          .content('play')
          .css({ display: 'none', 'margin-right': '20px' })
          .addTo(transport)
          .on('click', () => {
            if (isPlaying) pauseAudio()
            else playAudio()
          })

        const picker = nn.create('input')
          .set({ type: 'file', accept: 'audio/*' })
          .addTo(transport)
          .on('change', () => {
            const file = event.target.files[0]
            if (file) handleAudioFile(file)
          })

        const canvas = nn.create('canvas').css({ 'margin-top': '10px' }).addTo(opts.ele)
        canvas.width = nn.width
        canvas.height = nn.height / 4
        const ctx = canvas.getContext('2d')

        const playhead = nn.create('div')
          .css({
            background: opts.colors.playhead,
            width: '1px',
            height: canvas.height,
            position: 'absolute',
            left: '0px',
            top: canvas.offsetTop
          })
          .set('class', 'audio-marker player-marker')
          .addTo(opts.ele)

        const loopStart = nn.create('div')
          .css({
            background: opts.colors.selectbars,
            height: canvas.height,
            top: canvas.offsetTop,
            display: 'none'
          })
          .set('class', 'loop-marker')
          .addTo(opts.ele)
          .on('mousedown', () => { loopStart.dataset.down = 'true' })
          .on('mouseup', () => { loopStart.dataset.down = 'false' })

        const loopEnd = nn.create('div')
          .css({
            background: opts.colors.selectbars,
            height: canvas.height,
            top: canvas.offsetTop,
            display: 'none'
          })
          .set('class', 'loop-marker')
          .addTo(opts.ele)
          .on('mousedown', () => { loopEnd.dataset.down = 'true' })
          .on('mouseup', () => { loopEnd.dataset.down = 'false' })

        nn.on('mousemove', (e) => {
          if (loopStart.dataset.down !== 'true' && loopEnd.dataset.down !== 'true') return

          const x = `${e.x}px` // NOTE: assumes canvas screenLeft
          const y  = `${canvas.offsetTop}px`
          if (loopEnd.dataset.down === 'true') {
            loopEnd.css({ left: x, top: y })
            let w = e.x - loopStart.x
            w = w >= 0 ? `${w}px` : '0px'
            loopArea.css({ left: loopStart.style.left, top: y, width: w })
          } else if (loopStart.dataset.down === 'true') {
            loopStart.css({ left: x, top: y })
            let w = loopEnd.x - e.x
            w = w >= 0 ? `${w}px` : '0px'
            loopArea.css({ left: x, top: y, width: w })
          }

          const duration = audioBuffer.duration
          loopInTime = (loopStart.x / canvas.width) * duration
          loopOutTime = (loopEnd.x / canvas.width) * duration
          timecode.content(`start: ${loopInTime}, end: ${loopOutTime}, duration: ${loopOutTime - loopInTime}`)
          if (player) {
            player.loopStart = loopInTime
            player.loopEnd = loopOutTime
          }
        })

        const loopArea = nn.create('div')
          .css({
            background: opts.colors.selectarea,
            opacity: 0.25,
            position: 'absolute',
            height: canvas.height,
            top: canvas.offsetTop,
            width: '0px'
          }).addTo(opts.ele)

        const timecode = nn.create('code')
          .addTo(opts.ele)

        nn.create('style').addTo('head').innerHTML = `
        .loop-marker {
          width: 1px;
          position: absolute;
          left: 0px;
          cursor: pointer;
        }
        .loop-marker:after {
          content: '';
          position: absolute;
          bottom: -10px;
          left: 50%;
          transform: translateX(-50%);
          border-left: 10px solid transparent;
          border-right: 10px solid transparent;
          border-bottom: 10px solid ${opts.colors.selectbars}
        }`
      }

      nn.on('load', () => {
        const cs = getComputedStyle(document.documentElement)
        const ele = '#waveform-sampler'
        const colors = {
          wave: cs.getPropertyValue('--accent-color1').trim(),
          playhead: cs.getPropertyValue('--text-color').trim(),
          selectbars: cs.getPropertyValue('--accent-color5').trim(),
          selectarea: cs.getPropertyValue('--accent-color2').trim()
        }
        createWFSampler({ ele, colors })
      })


    </script>
  </body>
</html>
