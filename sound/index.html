<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Algorithmic Music Online: Sound</title>
    <meta name="author" content="Nick Briz">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/png" href="/images/favicon.png" />
    <!-- TODO: social media stuff -->
    <link rel="stylesheet" href="/css/main.css">
    <style>
      .diagram {
        display: block;
        margin: 3em auto;
        aspect-ratio: 4 / 1;
        font-family: 'Libre Baskerville', Georgia, 'Times New Roman', serif;
        font-size: 0.8em;
        line-height: 0.6em;
        user-select: none;
      }

      am-range, am-button {
        display: block;
        width: 50%;
        margin: 0 auto;
        text-align: center;
      }

      .range-label {
        text-align: center;
        margin-bottom: 2em;
        margin-top: 1em;
      }
    </style>
  </head>
  <body>

    <main-menu>
      <div slot="menu-items">
        <div class="color-mode-wrap">
          <label class="color-mode-switch">
            <input type="checkbox">
            <span class="color-mode-slider"></span>
          </label>
          <span class="color-mode-label">light/dark mode</span>
        </div>
      </div>
    </main-menu>

    <div class="loader">
      <svg id="loader-wave" viewBox="0 0 100 100" preserveAspectRatio="none">
        <path d="" id="loader-wave-path"></path>
      </svg>
    </div>

    <section id="prologue">
      <div class="content">
        <h4 class="ch">chapter 1</h4>
        <h2 class="formatted-text ch-title">sound</h2>

        <svg id="preSineWave"></svg>

        <p>
          How can we create music with a computer? One way is to use a DAW (digital audio workstation), programs like Ableton or FL Studio. These apps are great when you want to create conventional music with ready-made tools. But when we want to push boundaries and tap the full potential of this <a href="http://www.newmediareader.com/book_samples/nmr-26-kay.pdf" target="_blank">meta-medium</a>, we turn to code. Instead of simply using software someone else wrote, we can program our own. Pre-made software gives us only the buttons, knobs, and menus its designers imagined. That can save time, but it also limits what and how we can create. By relying on someone else's GUI (graphical user interface), we restrict our imagination to their design choices, rather than exploring the much larger creative possibilities of the sounds a computer can make and how we might play it.
        </p>

        <p>
          Learning to make sound and music with code isn't just about getting programs to run, it's about understanding the principles that make them possible. The ideas you can imagine in code grow directly from your grasp of core concepts. If you skip that foundation and lean only on tools (whether pre-made software or even generative AI) you'll tend to produce work bounded by what you already know or what the tool suggests. It may feel productive, but without a sense of how and why this code works, and the history of ideas behind it, you lose the agency to shape it into something truly your own. Invest in the fundamentals, history, and theory, and your imagination will expand alongside your technical craft. The more time you spend experimenting and wrestling with the material, the more you'll tap the true potential of the computer as a creative medium, and the more interesting and original your work will become.
        </p>
        <br>
        <br>

        <h3 class="formatted-text">what is sound</h3>

        <p>
          Before we talk about music, we need to talk about sound, what it is and how we’ll be making it. Sound happens when something vibrates, like a vocal chord, a guitar string or a speaker. These vibrations cause the tiny particles in the air around it to move, creating patterns of squished-together areas (compressions) and spread-out areas (rarefactions). These patterns travel outward as waves. When these waves reach your ears, they make your eardrum vibrate in the same pattern. Your brain takes these vibrations and processes them into what you experience as sound—like speech, music or noise. In a way, sound only exists in your mind: it's your brain's way of interpreting the physical vibrations in the air into a meaningful hallucination. Without a listener, “sound” is nothing more than patterns of vibrations in the air.
        </p>

        <svg id="physicalWave" class="diagram" viewBox="0 0 650 205" preserveAspectRatio="xMidYMid meet"></svg>

        <am-range id="speed" min="0" max="100" step="1"></am-range>
        <div class="range-label">vibration speed (aka "frequency" or "pitch")</div>

        <am-range id="intensity" min="0" max="100" step="1"></am-range>
        <div class="range-label">vibration intensity (aka "amplitude" or "volume")</div>

        <br><br>

        <p>
          The pitch of a sound depends on how fast the particles are vibrating, and the volume depends on how much they’re moving. A sound's <strong>pitch</strong> is also called its <strong>frequency</strong>, which is measured in <strong>hertz (Hz)</strong> or cycles per second. Faster vibrations create higher frequencies and higher-pitched sounds, while slower vibrations create lower frequencies and lower-pitched sounds. The <strong>volume</strong> (loudness) of a sound depends on the wave's <strong>amplitude</strong>—how strong or "tall" the wave is. Higher amplitude means louder sounds, and it's measured in <strong>decibels (dB)</strong>. A decibel is a unit that measures how loud a sound is, using a scale where every 10 dB means the sound is 10 times more intense than the previous level. In short, amplitude controls loudness, and frequency controls pitch.
        </p>

        <p>
          We'll be generating sound by vibrating speakers attached to our computer. These could be small headphone speakers or larger blue-tooth connected speakers.
        </p>
        <br>
        <br>
        <br>

        <h3 class="formatted-text">digital sound</h3>

        <p>
          So we'll rephrase the initial question: How can we vibrate our speakers (ie. create sound) using code on a computer? Many languages and environments can create sound. Because our goal is to publish pieces on the open web, we'll use JavaScript, the Web's de facto programming language. Websites are built on <a href="https://developer.mozilla.org/en-US/docs/Web/HTML" target="_blank">HTML</a>, a beginner-friendly language for structuring content. We won't spend much time on HTML in this class, but if you're comfortable with HTML (and <a href="https://developer.mozilla.org/en-US/docs/Web/CSS" target="_blank">CSS</a>) you're welcome to use them in your projects. Our focus will be JavaScript. To run it in the browser, we'll put our code inside a <code>&lt;script&gt;</code> tag in an HTML file. Running JavaScript in a web browser gives us access to the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API" target="_blank">Web Audio API</a>, a set of audio building blocks for generating and shaping sound from scratch.
        </p>


      </div>
    </section>

    <section id="editor-0"></section>

    <hr>
    <br>
    <br>
    <br>

    <section>
      <div class="content">
        <h3 class="formatted-text">layers of abstractions</h3>

        <p>
          In the last of the examples above, we created a function called <code>playTone()</code> which abstracts the logic for creating an audio buffer (the array of values determining the speaker cone's excursion, aka displacement/deflection) into a higher level concept: playing a tone. This function can be controlled by passing it two values, one representing the pitch of the tone (the sound wave's frequency) and the other the volume of the tone (the sound wave's amplitude). Abstraction is choosing what details to ignore so you can think at the right level. If we were to give our program a GUI (graphical user interface), itself another layer of abstraction, it might look like this...
        </p>
        <br>

        <am-button id="play-tone">play tone</am-button>
        <p class="small-note">(waveform not to scale, slowed down for demonstration)</p>
        <svg id="sineWave"></svg>

        <!-- <am-range id="freq" min="27.5" max="4186.009" step="1"></am-range> -->
        <am-range id="freq" min="220" max="880" step="1"></am-range>
        <div class="range-label">pitch <span id="freq-val">440</span> hz</div>

        <am-range id="vol" min="0" max="10" step="0.01"></am-range>
        <div class="range-label">volume <span id="vol-val">5</span></div>

        <p>
          Computing is built from layers of abstraction, often called a "<strong>stack</strong>." At the bottom, electrons flow through transistors, encoding bits (1s and 0s) that the CPU turns into machine instructions. Above that sit the operating system and audio drivers that manage timing and move sample data to your sound hardware (which drive your speakers). In the browser, the JavaScript engine and the Web Audio API provide higher-level building blocks; our own functions sit above those, and at the top the GUI translates gestures into parameter changes. Each layer hides complexity and offers a different vocabulary for making sound. Creative agency comes from knowing where you are in that stack, what each layer affords, and when to step down a level to regain precision or invent something new.
        </p>

        <p>
          So far we've been working in the middle of the stack with JavaScript, talking directly to the browser's low-level Web Audio API. Next, we'll layer in libraries that let us think more musically and visually. <a href="https://tonejs.github.io/" target="_blank">Tone.js</a> abstracts Web Audio into higher-level instruments and signal flow (e.g., Synth, Filter, Gain), a musical time system ('4n', BPM, bars), and a reliable scheduler (Transport) so we can focus on notes, rhythms, and timbre instead of coding all that logic ourselves. We're going to keep experimenting with creating audio buffers from scratch for now, but know that later Tone.js will add all these laeyrs of abstraction to our stack of tools. For interfaces, we'll be using the <a href="https://github.com/netizenorg/netnet-standard-library?tab=readme-ov-file#netnet-standard-library" target="_blank">netnet-standard-library</a> (or <code>nn.min.js</code> for short) which will privde some music theory functions (for creating modes, scales, chords) and also streamlines the browser's <a href="https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model" target="_blank">DOM API</a> so building sliders, buttons, and simple UIs is quick and clear. We'll use these abstractions for most of the course to move faster and explore ideas, but keep in mind that you can always drop down to raw Web Audio or vanilla DOM API when you need finer control.
        </p>
      </div>
    </section>

    <section id="editor-1"></section>

    <hr>
    <br>
    <br>
    <br>

    <section>
      <div class="content">
        <h3 class="formatted-text">timbre</h3>

        <p>
          In the examples above we created two different sounds entirely from scratch, by calculating the raw data for each (the sample values to fill the audio buffer). These two sounds in some sense are opposites. The first was completely random, pure “noise” like the sound of a consonant in speech or the crash symbol on a drum set. The second was a “tone” like the sound of a vowel in speech or a key on a piano. It’s actually the purest tone we could make, the sound of a “sine wave”.
        </p>

        <p>
          However, pure sine waves don’t exist in the natural world, they can only ever be synthesized electronically (be it analog or digital). Most "natural" sounds with repetitive or predictable vibrations—what we hear as musical tones—are a mix of a <strong>fundamental</strong> frequency (the dominant tone, like 440 Hz for A4) and additional frequencies called <strong>overtones</strong> or <strong>harmonics</strong>. These overtones are multiples of the fundamental and give the sound its complexity.
        </p>

        <p>
          This mix of frequencies creates a sound’s <strong>timbre</strong>, the unique quality or texture that makes a guitar and a piano playing the same note sound different. While they share the same fundamental frequency, their overtones are emphasized differently, giving each instrument its distinctive "color." Overtones are what make musical tones rich and varied, compared to the pure simplicity of a sine wave.
        </p>

        <div id="timbre-ui" style="transform: translateY(3em); display: flex; align-items: center;"></div>
        <svg id="timbre-wave"></svg>
        <p class="small-note" style="margin-top: -4em; text-align: left;"><span style="font-weight:bold;">above</span>: waveform (shape of vibration) | <span style="font-weight:bold;">below</span>: spectrum analyzer (fundamental frequency and harmonics)</p>
        <svg id="timbre-spec"></svg>

        <p>
          In digital audio, we often describe four basic wave types, each with unique characteristics. Sine waves are pure tones, containing only the fundamental frequency with no harmonics, making them smooth and simple. Square waves are richer, combining the fundamental with only the odd-numbered harmonics (e.g., 3rd, 5th, 7th), giving them a buzzy, hollow sound. Saw waves are even more complex, containing the fundamental and all harmonics, which creates a bright, edgy sound often used in synth music. Triangle waves are similar to square waves but softer; they also include only odd harmonics, but the higher harmonics are much quieter, resulting in a more subdued, rounded tone. Each wave shape has a distinct frequency content, shaping its sound and timbre.
        </p>
      </div>
    </section>

    <section id="editor-2"></section>

    <hr>
    <br>
    <br>
    <br>

    <section>
      <div class="content">
        <h3 class="formatted-text">a note on these visualizations</h3>

        <p>
          The examples above all make use of a couple of helper functions for visualizing our audio, <code>createWaveform()</code> and <code>createSpectrum()</code>, this lesson isn't about audio visualization, but if you're curious to see the code behind those functions you can view the source code for both the <a href="/js/create-waveform.js" target="_blank">createWaveform</a> and <a href="/js/create-spectrum.js" target="_blank">createSpectrum</a> functions if you'd like. Visualizations like these are  created using an algorithm called <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform" target="_blank">the Fast Fourier Transform</a> or FFT for short. This algorithm has a very interesting history worth <a href="https://www.youtube.com/watch?v=nmgFG7PUHfo" target="_blank">learning more about</a>. I might also suggest you can check out Jack Schaedler's <a href="https://jackschaedler.github.io/circles-sines-signals/dft_introduction.html" target="_blank">interactive page on the Fourier Transform</a> for a more intuitive understanding of how adding/removing harmonics to a fundamental frequency changes the wave shape. The Web Audio API has a built-in implementation of the FFT algorithm in AnalyserNode which I <a href="/editor/#Web Audio API/the basics/Audio Analyser Node">explain here</a>, as well as how to use it to create visualizations <a href="/editor/#Web Audio API/the basics/visualization: canvas">with canvas</a> using the Web's native APIs. Of course, Tone.js also has it's higher level abstractions of this AnalyserNode which makes it easier to create visuals with the same FFT algorithms running behind the scenes, I've also created examples for these including how to create a <a href="/editor/#nn.min.js/visuals%20(analysis)/volume%20meter" target="_blank">volume meter</a>, how to draw a <a href="/editor/#nn.min.js/visuals%20(analysis)/waveform" target="_blank">waveform</a> as well as <a href="/editor/#nn.min.js/visuals%20(analysis)/frequency%20bars" target="_blank">frquency spectrum bands</a>.
        </p>
        <br>
        <br>
        <hr>
        <br>
        <br>

        <p>
          That said, our main goal with this lesson is not to learn how to create sound visualizations (we'll revisit this later). Our goal is to understand one of the most fundamental principles of digital audio: sound buffers. Over the next few weeks we'll start climbing the ladder of abstraction and begin expressing sounds at a higher level (eventually in terms of musical notes), but before we do it's worth spending some time to experiment with buffers at this lower level, this will not only help in your fundamental understanding of what digital audio <i>is</i> and how it works, but might also plant a seed for ideas you might have in the future which could benefit from dipping back down into this lower level.
        </p>
      </div>
    </section>


    <section class="attribution">
      <div class="content">
        <p>
          <span style="font-weight: bold;">Attribution</span>: Text and code written by <a href="https://nickbriz.com/" target="_blank">Nick Briz</a>. The code editor icons designed by <a href="https://thenounproject.com/creator/MekoDa/" target="_blank">Meko</a> and licensed under Creative Commons Attribution License (CC BY 3.0). Air pressure and sine wave diagram remixed from Jack Schaedler's <a href="https://jackschaedler.github.io/circles-sines-signals" target="_blank">Seeing Circles, Sines and Signals</a>. All sounds generated using the Web Audio API and/or <a href="https://tonejs.github.io/" target="_blank">Tone.js</a> by Yotam Mann and <a href="https://github.com/Tonejs/Tone.js/graphs/contributors" target="_blank">other contributors</a>.
        </p>
      </div>
    </section>


    <script src="/js/libs/d3@7.js"></script>
    <script src="/js/libs/Tone.js"></script>
    <script src="/js/libs/netitor.min.js"></script>
    <script src="/js/libs/nn.min.js"></script>
    <script src="/js/custom-elements/main-menu.js"></script>
    <script src="/js/custom-elements/algo-music-ui.js"></script>
    <script src="/js/jack/sound_wave.js"></script>
    <script src="/js/jack/sound-1.js"></script>
    <script src="/js/SVGSineWave.js"></script>
    <script src="/js/create-waveform.js"></script>
    <script src="/js/create-spectrum.js"></script>
    <script src="/js/code-templates.js"></script>
    <script src="/js/utils.js"></script>
    <script>
      utils.init()
      nn.getAll('h4, .formatted-text').forEach(e => utils.formatText(e))

      // sound vibration diagram
      nn.get('#speed').value = 12
      nn.get('#speed').on('input', () => {
        const v = parseFloat(nn.get('#speed').value)
        window.updateSpeed(v)
      })
      nn.get('#intensity').value = 50
      nn.get('#intensity').on('input', () => {
        const v = parseFloat(nn.get('#intensity').value)
        window.updateAmount(v)
      })

      // code examples
      // --------------------------------------------

      utils.createCodeEditor({
        ele: '#editor-0',
        title: 'raw audio data',
        code: [
          { file: 'sound-buffers/sound-1.html', info: true },
          { file: 'sound-buffers/sound-2.html', info: true },
          { file: 'sound-buffers/sound-3.html', info: true },
          { file: 'sound-buffers/sound-4.html', info: true },
          { file: 'sound-buffers/sound-5.html', info: true },
          { file: 'sound-buffers/sound-6.html', info: true },
          { file: 'sound-buffers/sound-7.html', info: true },
          { file: 'sound-buffers/sound-8.html', info: true }
        ]
      })

      utils.createCodeEditor({
        ele: '#editor-1',
        title: 'tone.js audio buffers',
        code: [
          { file: 'sound-tone-buffers/1.html', info: true },
          { file: 'sound-tone-buffers/2.html', info: true },
          { file: 'sound-tone-buffers/3.html', info: true },
          { file: 'sound-tone-buffers/4.html', info: true },
          { file: 'sound-tone-buffers/5.html', info: true }
        ]
      })

      const ttemps = ['body', 'nn', 'tone', 'viz']
      utils.createCodeEditor({
        ele: '#editor-2',
        title: 'sound wave buffers',
        code: [
          { file: 'sound-tone-buffers/6.js', template: ttemps, info: true },
          { file: 'sound-tone-buffers/7.js', template: ttemps, info: true },
          { file: 'sound-tone-buffers/8.js', template: ttemps, info: true },
          { file: 'sound-tone-buffers/9.js', template: ttemps, info: true },
          { file: 'sound-tone-buffers/10.js', template: ttemps, info: true }
        ]
      })

      // sine wave diagram
      // --------------------------------------------
      const preSineWave = new SVGSineWave({
        svg: '#preSineWave',
        frequency: 4,
        amplitude: 0.5,
        colors: ['var(--text-color)', 'var(--accent-color3)'],
        width: nn.get('.content').width
      })

      const sineWave = new SVGSineWave({
        svg: '#sineWave',
        frequency: 12,
        amplitude: 0.5,
        colors: ['var(--text-color)', 'var(--accent-color1)'],
        width: nn.get('.content').width
      })

      nn.on('resize', () => {
        sineWave.update({ width: nn.get('.content').width })
        preSineWave.update({ width: nn.get('.content').width })
      })

      nn.get('#freq').value = 440
      nn.get('#freq').on('input', () => {
        const v = parseFloat(nn.get('#freq').value)
        sineWave.update({ frequency: nn.map(v, 440, 880, 12, 24) })
        nn.get('#freq-val').content(nn.get('#freq').value)
      })
      nn.get('#vol').value = 5
      nn.get('#vol').on('input', () => {
        const v = parseFloat(nn.get('#vol').value)
        sineWave.update({ amplitude: nn.map(v, 0, 10, 0, 1) })
        nn.get('#vol-val').content(nn.get('#vol').value)
      })

      nn.get('#play-tone').on('click', () => {
        const ctx = new window.AudioContext()
        const lvl = ctx.createGain()
        lvl.gain.value = parseFloat(nn.get('#vol').value) / 10
        const tone = ctx.createOscillator()
        tone.frequency.value = nn.get('#freq').value
        tone.connect(lvl)
        lvl.connect(ctx.destination)

        tone.start(ctx.currentTime)
        tone.stop(ctx.currentTime + 0.25)
      })

      // timbre diagram
      // --------------------------------------------
      const wave = createWaveform({
        ele: '#timbre-wave',
        background: 'var(--background-color)',
        color: 'var(--accent-color1)'
      })

      const spec = createSpectrum({
        ele: '#timbre-spec',
        background: 'var(--background-color)',
        color: 'var(--accent-color1)',
        harmonics: true,
        binSize: 1024 * 2,
        range: [20, 7050]
      })

      const osc = new Tone.Oscillator(440, "sine").toDestination()
      osc.connect(wave)
      osc.connect(spec)

      nn.create('button')
        .addTo('#timbre-ui')
        .content('start')
        .on('click', () => {
          osc.start()
        })

      nn.create('button')
        .addTo('#timbre-ui')
        .content('stop')
        .on('click', () => {
          osc.stop()
        })

      nn.create('select')
        .set({ id: 'wave-types' })
        .addTo('#timbre-ui')
        .on('input', (e) => {
          osc.type = e.target.value
        })

      const types = ['sine', 'square', 'triangle', 'sawtooth']
      types.forEach(type => {
        nn.create('option')
        .content(type)
        .set({ value: type })
        .addTo('#wave-types')
      })

      nn.create('input')
        .set({ type: 'number', value: 0, min: 0, step: 1 })
        .css({ width: '6em' })
        .addTo('#timbre-ui')
        .on('input', (e) => {
          const type = nn.get('#wave-types').value
          const vals = e.target.value
          const part = vals !== '' && vals > 0 ? type + vals : type
          osc.type = part
          console.log(osc.type);
        })

      nn.create('span')
        .content('&nbsp;&nbsp;&nbsp;volume: ')
        .addTo('#timbre-ui')

      nn.create('am-range')
        .set({ value: 0, min: -100, max: 0, step: 1 })
        // .css({ width: '6em' })
        .addTo('#timbre-ui')
        .on('input', (e) => {
          osc.volume.value = Number(e.target.value)
        })


    </script>
  </body>
</html>
